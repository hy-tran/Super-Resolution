{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a086c2c",
   "metadata": {},
   "source": [
    "The next cell is to check if GPU is available in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e23d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bhtran/repo/Super-Resolution/.venv/bin/python\n",
      "/home/bhtran/repo/Super-Resolution/.venv/lib/python3.12/site-packages/torch/__init__.py\n",
      "2.9.1+cu126\n",
      "12.6\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "print(sys.executable)\n",
    "print(torch.__file__)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d8db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba448054",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "We will create a custom dataset class for loading our images. This class will inherit from `torch.utils.data.Dataset` and implement the necessary methods to load and preprocess the images (and maybe data augmentation in the future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e60a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, npz_file_path):\n",
    "        \"\"\"Load tensors from .npz file\"\"\"\n",
    "        data = np.load(npz_file_path)\n",
    "        self.hr_images = data['hr']\n",
    "        self.lr_images = data['lr']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hr_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Because the image dimension is (batch, height, width, channels) \n",
    "        #So i do a permutation here to (C,H,W) to fit the model\n",
    "        lr = torch.from_numpy(self.lr_images[idx]).permute(2, 0, 1)\n",
    "        hr = torch.from_numpy(self.hr_images[idx]).permute(2, 0, 1)\n",
    "\n",
    "        return lr, hr\n",
    "\n",
    "training_dataset = SuperResolutionDataset('../data/hr_lr_images.npz')\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f40ff",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "Split the dataset into training and validation sets to evaluate model performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d598bb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 13883\n",
      "Validation samples: 2603\n",
      "Test samples: 868\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split dataset into training (80%), validation (15%), and test (5%)\n",
    "train_size = int(0.8 * len(training_dataset))\n",
    "val_size = int(0.15 * len(training_dataset))\n",
    "test_size = len(training_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(training_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create dataloaders for training, validation, and test\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Validation samples: {len(val_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "573ba46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from src.model import SuperResolutionModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae7b75",
   "metadata": {},
   "source": [
    "The line below just a sanitty check to ensure that our model is working as expected. Sometimes notebooks environments are populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd32284e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "model = SuperResolutionModel()\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "y = model(x)\n",
    "print(type(y))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d51c5db",
   "metadata": {},
   "source": [
    "### Check your model shape and size\n",
    "The model should not exceed 5 million parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed07113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           4,864\n",
      "              ReLU-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,928\n",
      "              ReLU-4           [-1, 64, 32, 32]               0\n",
      "            Conv2d-5           [-1, 48, 32, 32]          27,696\n",
      "      PixelShuffle-6          [-1, 3, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 69,488\n",
      "Trainable params: 69,488\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.75\n",
      "Params size (MB): 0.27\n",
      "Estimated Total Size (MB): 3.03\n",
      "----------------------------------------------------------------\n",
      "Trainable parameters: 69488\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           4,864\n",
      "              ReLU-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,928\n",
      "              ReLU-4           [-1, 64, 32, 32]               0\n",
      "            Conv2d-5           [-1, 48, 32, 32]          27,696\n",
      "      PixelShuffle-6          [-1, 3, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 69,488\n",
      "Trainable params: 69,488\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.75\n",
      "Params size (MB): 0.27\n",
      "Estimated Total Size (MB): 3.03\n",
      "----------------------------------------------------------------\n",
      "Trainable parameters: 69488\n"
     ]
    }
   ],
   "source": [
    "# device refers to the GPU or the CPU, depending whether GPU is available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "model = SuperResolutionModel().to(device)\n",
    "\n",
    "# display information about the model\n",
    "summary(model, (3,32,32))\n",
    "\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Trainable parameters:', params)\n",
    "if params > 5_000_000:\n",
    "    raise Exception('Your model is unecessarily complex, scale down!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584da98",
   "metadata": {},
   "source": [
    "### Plot setup and Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae1f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback for plotting loss and accuracy during training\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "class PlotLogAccuracy:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.epochs = []\n",
    "    self.train_losses = []\n",
    "    self.val_losses = []\n",
    "    self.train_acc = []\n",
    "    self.val_acc = []\n",
    "    self.epoch_count = 0\n",
    "\n",
    "  def update(self, train_loss, train_acc, val_loss, val_acc):\n",
    "    self.epochs.append(self.epoch_count)\n",
    "    self.train_losses.append(train_loss)\n",
    "    self.val_losses.append(val_loss)\n",
    "    self.train_acc.append(train_acc)\n",
    "    self.val_acc.append(val_acc)\n",
    "    self.epoch_count += 1\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(self.epochs, self.train_losses, label=\"train loss\")\n",
    "    plt.plot(self.epochs, self.val_losses, label=\"validation loss\")\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('Model Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(self.epochs, self.train_acc, label=\"training accuracy\")\n",
    "    plt.plot(self.epochs, self.val_acc, label=\"validation accuracy\")\n",
    "    plt.legend()\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.show();\n",
    "\n",
    "plotter = PlotLogAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bff061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# setting the model (calling this again will reset the weights)\n",
    "model = SuperResolutionModel().to(device)\n",
    "\n",
    "# play with this\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=1e-4)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                         num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "valloader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                         num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                         num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "train_loss_hist, val_loss_hist = [], []\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b98f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d4df35876f4989be21d1dada783352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 [Train]:   0%|          | 0/217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m     preds = out.argmax(\u001b[32m1\u001b[39m)\n\u001b[32m     28\u001b[39m     total += y_batch.size(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     correct += (\u001b[43mpreds\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m).sum().item()\n\u001b[32m     31\u001b[39m train_loss = running_loss / \u001b[38;5;28mlen\u001b[39m(trainloader.dataset)\n\u001b[32m     32\u001b[39m train_acc = correct / total\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (64) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "print('Starting training...')\n",
    "\n",
    "# Epoch Loop: It iterates through the specified number of training epochs.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # For each epoch, all data in the dataset is processed.\n",
    "\n",
    "    model.train() # Sets the model to training mode\n",
    "\n",
    "    running_loss = 0.0 # resetting loss metric\n",
    "\n",
    "    #  Iterates through the `trainloader` to get mini-batches of data\n",
    "    for x_batch, y_batch in tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs} [Train]', leave=False):\n",
    "\n",
    "        # copying data to the GPU (the `device`) if GPU is available\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()           # setting up gradient to zero\n",
    "        out = model(x_batch)            # Performs the forward pass\n",
    "        loss = criterion(out, y_batch)  # calculates the loss\n",
    "        loss.backward()                 # performs backpropagation in parallel on the batch\n",
    "        optimizer.step()                # optimizer steps to update the model's weights\n",
    "\n",
    "        # updating current training loss on the mini-batch\n",
    "        running_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(trainloader.dataset)\n",
    "\n",
    "    # Computing Validation Loss\n",
    "\n",
    "    model.eval() # switching model to eval mode, disabling dropout/batchnorm/other custom modules\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad(): # we disable gradient computation to save some memory\n",
    "        for x_batch, y_batch in tqdm(valloader, desc=f'Epoch {epoch+1}/{epochs} [Val]', leave=False):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            out = model(x_batch)\n",
    "            loss = criterion(out, y_batch)\n",
    "            val_running_loss += loss.item() * x_batch.size(0)\n",
    "    val_loss = val_running_loss / len(valloader.dataset)\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "    # Record training loss\n",
    "    train_loss_hist.append(train_loss); val_loss_hist.append(val_loss)\n",
    "\n",
    "    # Print loss for training/validation\n",
    "    print(f'Epoch {epoch+1}: Train Loss {train_loss:.4f} | Val Loss {val_loss:.4f}')\n",
    "\n",
    "print('Training finished.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
